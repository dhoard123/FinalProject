{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MelanomaNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mQndBOnCLmqz",
        "outputId": "396f27ed-2022-4cfc-9ff0-b9a64a59ddb7"
      },
      "source": [
        "!git clone \"https://github.com/dhoard123/FinalProject.git\"\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'FinalProject'...\n",
            "warning: You appear to have cloned an empty repository.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cc9gtp8o7fgg"
      },
      "source": [
        "IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhPgExAktFHd"
      },
      "source": [
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from skimage.transform import resize\n",
        "import cv2\n",
        "import scipy\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, ToTensor, Resize\n",
        "from torch.utils.data import Subset\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data.sampler import SubsetRandomSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3NcMGzb7j0p"
      },
      "source": [
        "PRE-PROCESS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGongNih7n3i"
      },
      "source": [
        "def get_data(csv_file):\n",
        "    \"\"\"\n",
        "    Load the data and labels from the given folder.\n",
        "    \"\"\"\n",
        "    X = []\n",
        "    y = []\n",
        "\n",
        "    data_info = pd.read_csv(csv_file, skiprows=1, header=None)\n",
        "    image_arr = np.asarray(data_info.iloc[:, 0])\n",
        "    label_arr = np.asarray(data_info.iloc[:, 7])\n",
        "    height = 200\n",
        "    width = 200\n",
        "\n",
        "    for index, image_name in enumerate(image_arr):\n",
        "        #print(index, str(BASE_DIR / 'images' / 'train' / str(image_name + '.jpg')))\n",
        "        img_file = cv2.imread(str(BASE_DIR / 'images' / 'train' / str(image_name + '.jpg')))\n",
        "        label = label_arr[index]\n",
        "        print(index, image_name, label)\n",
        "\n",
        "        if img_file is not None:\n",
        "            img_file = resize(img_file, (height, width, 3), order=1)\n",
        "            img_np_arr = np.asarray(img_file)\n",
        "            X.append(img_np_arr)\n",
        "            y.append(int(label))\n",
        "    \n",
        "    X = np.asarray(X)\n",
        "    y = np.asarray(y)\n",
        "    return X,y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Goc4JXb67-OW"
      },
      "source": [
        "TRAIN OR EVALUATE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O019bRhO8IR5",
        "outputId": "77d0ad71-16a2-49e7-ad09-3ac06fba6b31"
      },
      "source": [
        "train_eval = str(input(\"Training mode (0) or eval mode [must have .npy files] (1)?: \"))\n",
        "while train_eval != '0' and train_eval != '1':\n",
        "    train_eval = str(input(\"Training mode (0) or eval mode [must have .npy files] (1)?: \"))\n",
        "train_eval = int(train_eval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training mode (0) or eval mode [must have .npy files] (1)?: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf4P1_8Q8KvO"
      },
      "source": [
        "HYPER PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K17Sbj_-8PGJ"
      },
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 5\n",
        "num_classes = 2\n",
        "batch_size = 25\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKdFC60l8TCy",
        "outputId": "c1713039-620a-49cc-fd6b-f9879a4c3f55"
      },
      "source": [
        "print(BASE_DIR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/CS184A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VNFKulL8x-b"
      },
      "source": [
        "TRAIN OR LOAD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tDrXuk6R85lk",
        "outputId": "49313118-b3fe-4313-cc47-2db161537bfe"
      },
      "source": [
        "if train_eval == 0:\n",
        "    \n",
        "    # Ask whether to run the get_data or load from existing files\n",
        "    save_load = str(input(\"[TAKES A LONG TIME] Create arrays and save (0) or load arrays from .npy files (1)?: \"))\n",
        "    while save_load != '0' and save_load != '1':\n",
        "        save_load = str(input(\"[TAKES A LONG TIME] Create arrays and save (0) or load arrays from .npy files (1)?: \"))\n",
        "    save_load = int(save_load)\n",
        "\n",
        "else:\n",
        "    save_load = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[TAKES A LONG TIME] Create arrays and save (0) or load arrays from .npy files (1)?: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8ooxns286zk"
      },
      "source": [
        "READ AND SAVE LABELED DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "OxaasJrH8-3M",
        "outputId": "cdff810a-2490-46b3-d244-5d8b2f469e59"
      },
      "source": [
        "if save_load == 0:\n",
        "    X_train, y_train = get_data(BASE_DIR / 'train.csv')\n",
        "    np.save(str(BASE_DIR / 'X_train.npy'), X_train)\n",
        "    np.save(str(BASE_DIR / 'y_train.npy'), y_train)\n",
        "\n",
        "    X_test, y_test = get_data(BASE_DIR / 'test.csv')\n",
        "    np.save(str(BASE_DIR / 'X_test.npy'), X_test)\n",
        "    np.save(str(BASE_DIR / 'y_test.npy'), y_test)\n",
        "\n",
        "    print(\"Preliminary data saved to\", str(BASE_DIR))\n",
        "elif save_load == 1:\n",
        "    X_train = np.load(str(BASE_DIR / 'X_train.npy'))\n",
        "    y_train = np.load(str(BASE_DIR / 'y_train.npy'))\n",
        "    X_test = np.load(str(BASE_DIR / 'X_test.npy'))\n",
        "    y_test = np.load(str(BASE_DIR / 'y_test.npy'))\n",
        "    print(\"Preliminary data loaded from\", str(BASE_DIR))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-79a1f8b08e2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Preliminary data saved to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0msave_load\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'X_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'y_train.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'X_test.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m         \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/hoard/Documents/AIMelanomaNN/siim-isic-melanoma-classification/X_train.npy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_pOudeQ9FSz"
      },
      "source": [
        "print()\n",
        "print(\"Training shape:\", y_train.shape, X_train.shape)\n",
        "print(\"Test shape:\",y_test.shape, X_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIZGKJeZ9N-L"
      },
      "source": [
        "encoder = LabelEncoder()  # convert labels to 0 to n-1\n",
        "encoder.fit(y_train)\n",
        "y_train = encoder.transform(y_train)\n",
        "y_test = encoder.transform(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9mk01K19OjZ"
      },
      "source": [
        "TRAIN AND TEST DATALOADER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTiQ_h4T9P2a"
      },
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_train.transpose(0,3,1,2)).float(), torch.from_numpy(y_train))\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.from_numpy(X_test.transpose(0,3,1,2)).float(), torch.from_numpy(y_test))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,                                           \n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size, \n",
        "                                          shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmbiJ0cs9S4_"
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVV-bR8p9WW5"
      },
      "source": [
        "Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKZiZNHJ9a_J"
      },
      "source": [
        "class ConvNet(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, kernel_size=5, stride=1, padding=2), # 3 x 200 x 200\n",
        "            #nn.BatchNorm2d(8),\n",
        "            #nn.ReLU(),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)) # 8 x 100 x 100\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 16, kernel_size=5, stride=1, padding=2), # 16 x 100 x 100\n",
        "            #nn.BatchNorm2d(32),\n",
        "            #nn.ReLU(),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)) # 16 x 50 x 50\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2), # 32 x 50 x 50\n",
        "            #nn.BatchNorm2d(32),\n",
        "            #nn.ReLU(),\n",
        "            nn.Sigmoid(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)) # 32 x 25 x 25\n",
        "        self.fc = nn.Linear(25*25*32, num_classes)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = out.reshape(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = ConvNet(num_classes).to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22PFJrDX9fyh"
      },
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "total_step = len(train_loader)\n",
        "test_step = len(test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ialPcSWB9i4g"
      },
      "source": [
        "TRAINING MODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tViYXycz9ksW"
      },
      "source": [
        "if train_eval == 0:\n",
        "    # Train the model\n",
        "    \n",
        "    for epoch in range(num_epochs):   \n",
        "        for i, (images, labels) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "    # AutoSave model\n",
        "    torch.save(model.state_dict(), BASE_DIR / \"model_3.pt\")\n",
        "\n",
        "    \n",
        "    # Test the model\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Test Accuracy of the model on the {} test images: {} %'.format((test_step * 25), (100 * correct / total)))\n",
        "\n",
        "\n",
        "    # Print model's state_dict\n",
        "    print(\"Model's state_dict:\")\n",
        "    for param_tensor in model.state_dict():\n",
        "        print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuSMPn6l9pq6"
      },
      "source": [
        "EVALUATION MODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVrSlH_n9rn6"
      },
      "source": [
        "elif train_eval == 1:\n",
        "    model = ConvNet(num_classes).to(device)\n",
        "    model.load_state_dict(torch.load(BASE_DIR / \"model_3.pt\"))\n",
        "\n",
        "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        print('Test Accuracy of the model on the ~{} test images: {} %'.format((test_step * 25), (100 * correct / total)))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}